# ğŸ§  AI Call Insights â€” Live Call Transcription & Sentiment Analysis (POC)

## ğŸ“‹ Overview
**AI Call Insights** is an end-to-end Proof-of-Concept for **real-time call transcription and sentiment analysis**.  
It demonstrates how a healthcare or insurance call-center system can transcribe live audio, analyze emotion, and expose an API for further integration.

The architecture uses:
- ğŸ™ï¸ **Whisper (OpenAI)** for speech-to-text (local or API)
- ğŸ’¬ **LLaMA-2 / LLaMA-3** for sentiment classification
- âš¡ **FastAPI** backend with WebSocket streaming
- ğŸ–¥ï¸ Simple frontend for testing live transcription

---

## ğŸ§© Features
- ğŸ”„ Real-time audio streaming via **WebSocket**
- ğŸ—£ï¸ On-the-fly **speech-to-text** transcription
- â¤ï¸ **Sentiment analysis** (Positive / Negative / Neutral)
- ğŸŒ API endpoints for integration with enterprise systems
- ğŸ§± Modular services â€” easy to switch between **local LLMs** and **cloud APIs**
- ğŸ“¦ Ready for Docker / Azure deployment

---

## ğŸ—ï¸ Project Structure
```
ai-call-insights/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app entrypoint
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py           # Environment variables and settings
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ transcribe.py       # REST endpoint for file transcription
â”‚   â”‚   â”œâ”€â”€ sentiment.py        # REST endpoint for sentiment analysis
â”‚   â”‚   â”œâ”€â”€ process_call.py     # Combined call-processing route
â”‚   â”‚   â””â”€â”€ realtime.py         # WebSocket route for live audio
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ whisper_service.py  # Local/Remote Whisper logic
â”‚   â”‚   â”œâ”€â”€ llama_service.py    # LLaMA or external API call
â”‚   â”‚   â””â”€â”€ realtime_service.py # Streaming queue and orchestration
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ audio_utils.py      # Audio helpers
â”‚   â”‚   â””â”€â”€ schemas.py          # Data models
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ realtime_test.html      # Simple WebSocket test page
â”‚
â”œâ”€â”€ .env.example                # Sample environment file
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Clone and enter project
```bash
git clone https://github.com/<your-org>/ai-call-insights.git
cd ai-call-insights
```

### 2ï¸âƒ£ Create a virtual environment
```bash
python -m venv .venv
.\.venv\Scripts\activate     # Windows
source .venv/bin/activate      # Linux/Mac
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
For local LLMs:
```bash
pip install openai-whisper pydub torch
```

If using **API models**, you can skip heavy dependencies.

---

## ğŸ§¾ .env Configuration
Copy `.env.example` to `.env` and set values:

```bash
HOST=127.0.0.1
PORT=8000

# Whisper (Speech-to-Text)
USE_LOCAL_WHISPER=True
WHISPER_MODEL=base
WHISPER_API_KEY=<if using external API>

# LLaMA / Sentiment Model
USE_LOCAL_LLAMA=True
LLAMA_MODEL=llama2
LLAMA_API_KEY=<if using external API>
```

---

## ğŸ§  Installing Local Models

### ğŸ—£ï¸ Option 1 â€” Local Whisper (OpenAI)
```bash
pip install openai-whisper
```
Whisper requires **ffmpeg**.  
- Download from: [https://www.gyan.dev/ffmpeg/builds/](https://www.gyan.dev/ffmpeg/builds/)  
- Extract to `C:\ffmpeg\bin`
- Add to `PATH` or set in `.env`:
  ```bash
  FFMPEG_PATH=C:\ffmpeg\bin\ffmpeg.exe
  FFPROBE_PATH=C:\ffmpeg\bin\ffprobe.exe
  ```

---

### ğŸ’¬ Option 2 â€” Local LLaMA (via Ollama)
Install [Ollama](https://ollama.ai/download).

Then pull the models:
```bash
ollama pull llama2
ollama pull sentiment
```

Test locally:
```bash
ollama run llama2 "Classify this as positive, negative, or neutral: Iâ€™m happy today."
```

Set in `.env`:
```bash
USE_LOCAL_LLAMA=True
LLAMA_MODEL=llama2
```

---

### â˜ï¸ Option 3 â€” Use APIs (Azure/OpenAI)
If you prefer external APIs (for production or scaling):

```bash
USE_LOCAL_WHISPER=False
USE_LOCAL_LLAMA=False

WHISPER_API_URL=https://api.openai.com/v1/audio/transcriptions
WHISPER_API_KEY=<your_key>

LLAMA_API_URL=https://api.groq.com/v1/chat/completions
LLAMA_API_KEY=<your_key>
```

These will be used automatically by the services.

---

## ğŸš€ Running Locally

### Backend
```bash
uvicorn app.main:app --reload
```

You should see:
```
âœ… Using ffmpeg from: C:\ffmpeg\bin\ffmpeg.exe
âœ… Using ffprobe from: C:\ffmpeg\bin\ffprobe.exe
INFO: Application startup complete.
```

### Frontend
Open in browser:
```
frontend/realtime_test.html
```
Then click **Start Streaming** to begin transcription.

---
